{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [  9.08345597e-05]\n",
      "[0 1] [ 0.99713889]\n",
      "[1 0] [ 0.99592761]\n",
      "[1 1] [ 0.00352495]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkgADQZLUGAiSJAD+H8J/itBnH8lgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f319a7bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "array  = []\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            array.append(abs(error))\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "        \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    plt.plot(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
